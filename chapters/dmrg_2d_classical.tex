\section{Partition functions of classical lattices}
The central quantity in equilibrium statistical mechanics is the partition
function $Z$, which, for a discrete system such as a lattice, is defined as
\begin{equation}
  Z = \sum_{s} \exp{(-\beta H(s))}
\end{equation}
where the sum is over all microstates $s$, $H$ is the energy function, and
$\beta = T^{-1}$ the inverse temperature.

\section{Transfer matrices of lattice models}

\subsection{1D Ising model}

\todo[inline]{refer to Ising, talk a bit about model (magnetism etc).}

Consider the 1D zero-field ferromagnetic Ising model \cite{ising1925beitrag}, defined by the energy function
\begin{equation}\label{ising_energy_function}
  H(\sigma) = -J \sum_{\langle i j \rangle} \sigma_i \sigma_j
\end{equation}
Here, we sum over nearest neighbors $\langle i j \rangle$ and the spins
$\sigma_i$ take the values $\pm 1$. $J > 0$.

Assume, for the moment, that the chain
consists of $N$ spins, and apply periodic boundary conditions.
The partition function of this system is given by
\begin{equation}
  Z_{N} = \sum_{\sigma_1, \dotsc, \sigma_N \in \{-1, 1\}} \exp (-\beta H(\sigma))
\end{equation}
Exploiting the local nature of the interaction between spins, we can write
\begin{equation}
  Z_{N} = \sum_{\sigma_1, \cdots, \sigma_N \in \{-1, 1\}} \prod_{\langle i, j \rangle} e^{K\sigma_i \sigma_j}
\end{equation}
where we defined $K \equiv \beta J$.

Now, we define the $2 \times 2$ matrix
\begin{equation}\label{eq:transfer_matrix_1d_ising}
  T_{\sigma \sigma'} = \exp(K \sigma \sigma')
\end{equation}

A possible
choice of basis is
\begin{equation}\label{eq:basis_transfer_matrix_1d}
  \bigl( \ket{\uparrow} = 1, \ket{\downarrow} = -1 \bigr) =
  \bigl(
  \begin{bmatrix}
    1 \\
    0
  \end{bmatrix},
  \begin{bmatrix}
    0 \\
    1
  \end{bmatrix}
  \bigr)
\end{equation}

In terms of this matrix, $Z_N$ is written as
\begin{equation}\label{eq:partition_function_transfer_matrix_1d}
  Z_N = \sum_{\sigma_1, \cdots, \sigma_N} T_{\sigma_1 \sigma_2} \dotsm T_{\sigma_N \sigma_1} = \tr T^N
\end{equation}

$T$ is called the transfer matrix. Since $T$ is, in fact, diagonalizable, $T^N = P D^N
P^{-1}$, where $P$ consists of the eigenvectors of $T$. By the cyclic property of the
trace, we have
\begin{equation}
  Z_N = \lambda_{1}^{N} + \lambda_{2}^{N}
\end{equation}

Thus, we have reduced the problem of finding the partition function to an
eigenvalue problem, which is quite easy in this case.

Note that in the thermodynamic limit $N \to \infty$
\begin{equation}
  Z = \lim_{N \to \infty} \lambda_{1}^{N}
\end{equation}
where $\lambda_1$ is the non-degenerate largest eigenvalue (in absolute value) of $T$.

\subsubsection{Fixed boundary conditions}\label{sec:fixed_boundary_conditions}
We may also apply fixed boundary conditions. The partition function is then written as
\begin{equation}
  Z_N = \bra{\sigma'}T^N\ket{\sigma},
\end{equation}
where $\ket{\sigma}$ and $\ket{\sigma'}$ are the right and left boundary spins.

In the large-$N$ limit, $T^N$ tends towards the projector onto the eigenspace spanned by
the eigenvector belonging to the largest eigenvalue
\begin{equation}\label{eq:largest_eigenvector}
  \ket{\lambda_1} = \lim_{N \to \infty} \frac{T^N \ket{\sigma}}{\left\lVert T^N \ket{\sigma} \right\rVert}.
\end{equation}

\autoref{eq:largest_eigenvector} is true for any $\ket{\sigma}$ that is not orthogonal to $\ket{\lambda_1}$.

The physical significance of the normalized lowest-lying eigenvector $\ket{\lambda_1}$ is
that
$\braket{\lambda_1 | \uparrow}$ and
$\braket{\lambda_1 | \downarrow}$ represent the Boltzmann weight of $\ket{\uparrow}$ and
$\ket{\downarrow}$ at the boundary of a half-infinite chain.

\subsection{2D Ising model}
\todo[inline]{Talk about exact solution (Onsager). Why is it important? Maybe
star-triangle relation (Baxter). Not all IRF models solvable.}

Next, we treat the two-dimensional, square-lattice Ising model. In two
dimensions, the energy function is still written as in
\autoref{ising_energy_function}, but now every lattice site has four neighbors.

Let $N$ be the number of columns and $l$ be the number of rows of the lattice, and assume
$l \gg N$. In the vertical direction, we apply periodic boundary conditions, as in the
one-dimensional case. In the horizontal direction, we keep an open boundary. We refer to
$N$ as the system size.

Similarly as in the 1D case, the partition function can be written as
\begin{equation}
  Z_N = \sum_{\bm{\sigma}} \prod_{\langle i, j, k, l \rangle} W(\sigma_i, \sigma_j, \sigma_k, \sigma_l)
\end{equation}
where the product runs over all groups of four spins sharing the same face. The Boltzmann weight of such a face is given by
\begin{equation}\label{eq:boltzmann_weight_face_ising_model}
  W(\sigma_i, \sigma_j, \sigma_k, \sigma_l) = \exp \left\{ \frac{K}{2} (\sigma_i \sigma_j + \sigma_j \sigma_k + \sigma_k \sigma_l + \sigma_l \sigma_i) \right\}
\end{equation}

We can express the Boltzmann weight of a configuration of the whole lattice as
a product of the Boltzmann weights of the rows
\begin{equation}
  Z_N = \sum_{\bm{\sigma}} \prod_{r = 1}^{l} W(\sigma_{1}^{r}, \sigma_{2}^{r}, \sigma_{1}^{r+1}, \sigma_{2}^{r+1}) \dots W(\sigma_{N-1}^{r}, \sigma_{N}^{r}, \sigma_{N-1}^{r+1}, \sigma_{N}^{r+1})
\end{equation}
where $\sigma_{i}^{r}$ denotes the value of the $i$th spin of row $r$.

Now, we can generalize the definition of the transfer matrix to two dimensions, by
defining it as the Boltzmann weight of an entire row
\begin{equation}\label{eq:row_to_row_transfer_matrix}
  T_{N}(\bm{\sigma}, \bm{\sigma'}) = W(\sigma_1, \sigma_2, \sigma_1', \sigma_2') \dots W(\sigma_{N-1}, \sigma_N, \sigma_{N-1}', \sigma_{N}')
\end{equation}
If we take the spin configurations of an entire row as basis vectors, $T_N$ can be written
as a matrix of dimensions $2^N \times 2^N$.

Similarly as in the one-dimensional case, the partition function now becomes
\begin{equation}\label{eq:z_n_times_infty}
  Z_N = \sum_{\bm{\sigma}} \prod_{r = 1}^{l} T_{N}(\bm{\sigma}^r, \bm{\sigma}^{r+1}) = \tr T_{N}^l
\end{equation}

In the limit of an $N \times \infty$ cylinder, the partition function is once again
determined by the largest eigenvalue\footnote{As in the 1D case, $T$ is symmetric, so it
is orthogonally diagonalizable.}.
\begin{equation}\label{largest_eigenvalue_transfer_matrix}
  Z_N = \lim_{l \to \infty} T_{N}^{l} = \lim_{l \to \infty} (\lambda_0)_{N}^{l}
\end{equation}

The partition function in the thermodynamic limit is given by
\begin{equation}
  Z = \lim_{N \to \infty} Z_N
\end{equation}

\section{Partition function of the 2D Ising model as a tensor network}
In calculating the partition function of 1D and 2D lattices, matrices of Boltzmann weights
like $W$ and $T$ play a crucial role. We have formulated them in a way that is valid for
any interaction-round-a-face (IRF) model, defined by
\begin{equation}
  H \thicksim \sum_{\langle i, j, k, l \rangle} W(\sigma_i, \sigma_j, \sigma_k,
  \sigma_l)
\end{equation}
where the summation is over all spins sharing a face. $W$ can contain 4-spin,
3-spin, 2-spin and 1-spin interaction terms. The Ising model is a special case of the IRF
model, with $W$ given by \autoref{eq:boltzmann_weight_face_ising_model}.

We will now express the partition function of the 2D Ising model as a tensor network. The
transfer matrix $T$ is redefined in the process. This allows us to visualize the equations
in a way that is consistent with the many other tensor network algorithms under research
today.

\todo[inline]{
    For any $l$, the trace over the transfer matrix is the same in both definitions of
    $T$. It is symmetric, so they should be related by a basis transformation. Is it,
    perhaps, exactly the basis in which the CTM is diagonal? }

\subsection{A system of four spins}

We define
\begin{equation}
  Q(\sigma_i, \sigma_j) = \exp(K \sigma_i \sigma_j)
\end{equation}
as the Boltzmann weight of the bond between $\sigma_i$ and $\sigma_j$. It is the
same as the 1D transfer matrix in \autoref{eq:transfer_matrix_1d_ising}.

The Boltzmann weight of a face $W$ decomposes into a product of Boltzmann weights of
bonds
\begin{equation}
  W(\sigma_i, \sigma_j, \sigma_k, \sigma_l) =
  Q(\sigma_i, \sigma_j)Q(\sigma_j, \sigma_l)Q(\sigma_l, \sigma_k)Q(\sigma_k, \sigma_i)
\end{equation}

It is now easy to see that the partition function is equal to the contracted tensor
network in \autoref{fig:tensor_network_4_sites}:
\begin{equation}\label{eq:tensor_network_4_sites}
  \begin{split}
    Z_{2 \times 2} & =
    \sum_{\sigma_1, \sigma_2, \sigma_3, \sigma_4} \sum_{a, b, c, d}
    \delta_{\sigma_1, a} Q(a, b) \delta_{\sigma_2, b} Q(b, c)
    \delta_{\sigma_3, c} Q(c, d) \delta_{\sigma_4, d} Q(d, a) \\
    & =
    \sum_{\sigma_1, \sigma_2, \sigma_3, \sigma_4} W(\sigma_1, \sigma_2, \sigma_3, \sigma_4)
  \end{split}
\end{equation}
where the Kronecker delta is defined as usual:
\begin{equation}
  \delta_{i j} =
  \begin{cases}
    1 & \text{if } i = j \\
    0 & \text{if } i \neq j
  \end{cases}
\end{equation}

\begin{figure}
  \input{images/tensor_network_4_sites.tikz}
  \caption{A tensor network representation of the partition function of the Ising model on
  a $2 \times 2$ lattice. See \autoref{eq:tensor_network_4_sites}.}
  \label{fig:tensor_network_4_sites}
\end{figure}


\subsection{Thermodynamic limit}
\begin{figure}
  \input{images/q_to_p.tikz}
  \caption{Graphical form of \autoref{eq:q_to_p}.}
  \label{fig:q_to_p}
\end{figure}

\begin{figure}
  \input{images/a_tensor.tikz}
  \caption{Graphical form of \autoref{eq:a_tensor}.}
  \label{fig:a_tensor}
\end{figure}

We define the matrix $P$ by
\begin{equation}\label{eq:q_to_p}
  P^2 = Q
\end{equation}
as in \autoref{fig:q_to_p}. This allows us to write the partition function of an arbitrary
$N \times l$ square lattice as a tensor network of a single recurrent tensor $a_{i j k
l}$, given by
\begin{equation}\label{eq:a_tensor}
  a_{i j k l} = \sum_{a, b, c, d} \delta_{a b c d} P_{i a} P_{j b} P_{k c} P_{l d}
\end{equation}
where the generalization of the Kronecker delta is defined as
\begin{equation}
  \delta_{i_1 \dots i_n} =
  \begin{cases}
    1 & \text{if } i_1 = \ldots = i_n \\
    0 & \text{otherwise}
  \end{cases}
\end{equation}

See \autoref{fig:a_tensor} and \autoref{fig:2d_ising_as_tensor_network}. At the edges and
corners, we define suitable tensors of rank 3 and 2, which we will also denote by $a$.
\begin{align*}
  a_{i j k} &= \sum_{a b c} \delta_{a b c} P_{i a} P_{j b} P_{k c} \\
  a_{i j} &= \sum_{a b} \delta_{a b} P_{i a} P_{j b}
\end{align*}

The challenge is to approximate this tensor network in the thermodynamic limit.

\begin{figure}
  \input{images/2d_ising.tikz}
  \caption{$Z_{N \times l}$ can be written as a contracted tensor network of $N \times l$
  copies of the tensor $a$.}
  \label{fig:2d_ising_as_tensor_network}
\end{figure}

\subsection{The transfer matrix as a tensor network}
\todo[inline]{Say something about reshaping legs.}

With our newfound representation of the partition function as a tensor network, we can
redefine the row-to-row transfer matrix from
\autoref{eq:row_to_row_transfer_matrix} as the tensor network expressed in
\autoref{fig:transfer_matrix_as_tensor_network}. For all $l$, it is still true that
\begin{equation}
  Z_{N \times l} = \tr T_{N}^{l} = \sum_{i = 1}^{2^N} \lambda_{i}^{l}
\end{equation}
so the eigenvalues must be the same. That means that the new definition of the transfer
matrix is related to the old one by a basis transformation
\begin{equation}
  T_{\text{new}} = P T_{\text{old}} P^{T}
\end{equation}

\begin{figure}
  \input{images/transfer_matrix.tikz}
  \caption{The definition of $T_N$ as a network of $N$ copies of the tensor $a$.}
  \label{fig:transfer_matrix_as_tensor_network}
\end{figure}

\section{Transfer matrix renormalization group}
There is a deep connection between quantum mechanical lattice systems in $d$ dimensions
and classical lattice systems in $d + 1$ dimensions. Via the imaginary time path integral
formulation, the partition function of a one-dimensional quantum system can be written as
the partition function of an effective two-dimensional classical system. The ground state
of the quantum system corresponds to the largest eigenvector of the transfer matrix of the
classical system.

For more on the quantum-classical correspondence, see
\autoref{chapter:correspondence_quantum_classical}.
\todo[inline]{I want to refer to Baxter's variational
approach anno 1968 somehwere.}


\subsection{The infinite system algorithm for the transfer matrix}
Nishino \cite{nishino1995density, nishino1996corner} was the first to apply density matrix
renormalization group methods in the context of two-dimensional classical
lattices.

Analogous to the infinite system DMRG algorithm for approximating the Hamiltonian of
quantum spin chains, our goal is to
approximate the transfer matrix in the thermodynamic limit as well as possible within a
restriced number of basis states $m$. We will do this by adding a single site at a time,
and truncating the dimension from $2m$ to $m$ at each iteration.

For simplicity, we assume that, at the start of the algorithm, the transfer matrix already
has dimension $m$. We call this transfer matrix $P_N$. A good choice of initial
transfer matrix is obtained by contracting a couple of $a$-tensors, until dimension $m$ is
reached. See \autoref{fig:tmrg_initial_half_row_transfer_matrix}.

To specify fixed instead of open boundary conditions, we may use as boundary tensor a
slightly modified version of the three-legged version of $a$, namely
\begin{equation}
  a_{i j k}^{\sigma} = \sum_{a b c} \delta_{\sigma a b c} P_{i a}P_{j b}P_{k c}.
\end{equation}
that represents an edge site with spin fixed at $\sigma$.

We enlarge the system with one site by contracting with an additional $a$-tensor,
obtaining $P_{N + 1}$. See the first network in
\autoref{fig:tmrg_add_site_and_renormalize}.

In order to find the best projection from $2m$ basis states back to $m$, we embed the
system in an environment that is the mirror image of the system we presently have. We call
this matrix $T_{2N + 2}$. It represents the transfer matrix of $2N + 2$ sites. We find the
largest eigenvalue and corresponding eigenvector, as shown in
\autoref{fig:tmrg_eigenvalue_equation}.

The equivalent of the \textit{reduced density matrix of a
block} in the classical case is:
\begin{equation}\label{eq:reduced_density_matrix_classical_case}
  \rho_{N + 1} = \sum_{\sigma_B} \braket{\sigma_B | \lambda_0}\braket{\lambda_0 |
  \sigma_B}
\end{equation}
where we have summed over all the degrees of freedom of one of the half-row transfer
matrices $P_{N+1}$. See the first step of \autoref{fig:tmrg_reduced_density_matrix}.

The optimal renormalization
\begin{equation}
  \widetilde{P}_{N+1} = O P_{N + 1} O^{\dagger}
\end{equation}
is obtained by diagonalizing $\rho_{N + 1}$ and keeping the eigenvectors corresponding
to the $m$ largest eigenvalues. See the second step of \autoref{fig:tmrg_reduced_density_matrix}.

With this blocking procedure, we can successively find
\begin{equation}
    P_{N + 1} \rightarrow P_{N + 2} \rightarrow \dots
\end{equation}
until we have reached some termination condition.

\begin{figure}
  \input{images/tmrg_initial_half_row_transfer_matrix.tikz}
  \caption{A good starting point for the half-row transfer $P_N$ is obtained by
  contracting a couple of $a$-tensors, until $P_N$ reaches dimension $m$.}
  \label{fig:tmrg_initial_half_row_transfer_matrix}
\end{figure}

\begin{figure}
  \input{images/tmrg_add_site_and_renormalize.tikz}
  \caption{In the first step, $P_{N + 1}$ is obtained by contracting the current half-row
  transfer matrix $P_N$ with an additional $a$-tensor.
  In the second step, $P_{N + 1}$ is truncated back to an $m$-dimensional matrix, with the
  optimal low-rank approximation given by keeping the basis states corresponding to the
  $m$ largest eigenvalues of the density matrix. See
  \autoref{fig:tmrg_reduced_density_matrix}. }
  \label{fig:tmrg_add_site_and_renormalize}
\end{figure}

\begin{figure}
  \input{images/tmrg_eigenvalue_equation.tikz}
  \caption{Equation for the lowest-lying eigenvector of the row-to-row transfer matrix $T_{2N +
  2}$.}
  \label{fig:tmrg_eigenvalue_equation}
\end{figure}

\begin{figure}
  \input{images/tmrg_reduced_density_matrix.tikz}
  \caption{Graphical form of
  \autoref{eq:reduced_density_matrix_classical_case}.
  In the second step, $\rho_{N + 1}$ is diagonalized and only the eigenvectors
  corresponding to the $m$ largest eigenvalues are kept.
  }
  \label{fig:tmrg_reduced_density_matrix}
\end{figure}

\subsection{Physical interpretation of the reduced density matrix}
Generalizing the remarks from \autoref{sec:fixed_boundary_conditions} to the
two-dimensional case, we see that the normalized lowest-lying eigenvector of the transfer
matrix $T_N$ contains the Boltzmann weights of spin configurations on the boundary of a
half-infinite $N \times \infty$ lattice.

Therefore, the classical equivalent of the quantum mechanical reduced density matrix,
given by \autoref{eq:reduced_density_matrix_classical_case}, and by the first network in
\autoref{fig:tmrg_reduced_density_matrix}, represents the Boltzmann weights of
configurations along a cut in an $N \times \infty$ lattice.

Nishino and Okunishi \cite{nishino1996corner}, drawing on ideas from Baxter, realized the
Boltzmann weights of configurations along this cut could be obtained by employing
\textit{corner transfer matrices}, making it unneccessary to solve the eigenvalue problem
in \autoref{fig:tmrg_eigenvalue_equation}. Their method, called the Corner Transfer Matrix
Renormalization Group method, consumes far less resources while maintaining precision. For
this reason, it is the method of choice for most of the simulations in this thesis.


\section{Corner transfer matrix renormalization group}
\subsection{Corner transfer matrices}

The concept of corner transfer matrices for 2D lattices was first introduced by
Baxter \cite{baxter1968dimers, baxter1978variational, baxter1982exactly}.
Whereas the row-to-row transfer matrix \autoref{eq:row_to_row_transfer_matrix}
corresponds to adding a row to the lattice, the corner transfer matrix adds
a quadrant of spins. It was originally defined by Baxter as

\begin{equation}\label{eq:corner_transfer_matrix}
  A_{\bm{\sigma}, \bm{\sigma'}} =
  \begin{cases}
    \sum \prod_{\langle i, j, k, l \rangle} W(\sigma_i, \sigma_j, \sigma_k, \sigma_l) & \text{if } \sigma_{1} = \sigma_{1}' \\
    0 & \text{if } \sigma_{1} \neq \sigma_{1}'
  \end{cases}
\end{equation}
Here, the product runs over groups of four spins that share the same face, and
the sum is over all spins in the interior of the quadrant.

In a symmetric and isotropic model such as the Ising model, we have
\begin{equation}
  W(a, b, c, d) = W(b, a, d, c) = W(c, a, d, b) = W(d, c, b, a)
\end{equation}
and the partition of an $N \times N$ lattice is expressed as
\begin{equation}\label{z_n_times_n}
  Z_{N \times N} = \tr A^4.
\end{equation}

In the thermodynamic limit, this partition function is equal to
the partition function of an $N \times \infty$ lattice, given by
\autoref{eq:z_n_times_infty}.

The matrix in \autoref{eq:reduced_density_matrix_classical_case}, containing the Boltzmann
weights of spins along a cut down the middle of an $N \times \infty$ system, is
\textit{approximated} by
\begin{equation}
  \rho = A^4.
\end{equation}
The difference is that $A^4$ represents a square system of size $N \times N$ with a
cut, instead of an $N \times \infty$ strip with a cut. In the thermodynamic limit, both
systems become the same.

\subsection{Corner transfer matrix as a tensor network}
Similarly to how we redefined the row-to-row transfer matrix
(\autoref{eq:row_to_row_transfer_matrix}) as the tensor network in
\autoref{fig:transfer_matrix_as_tensor_network}, we can redefine the corner transfer
matrix (\autoref{eq:corner_transfer_matrix}) as the tensor network in
\autoref{fig:corner_transfer_matrix_as_tensor_network}. Again, the new and old definitions
of $A$ are related by a basis tranformation
\begin{equation}
  A_{\text{new}} = P A_{\text{old}} P^T.
\end{equation}

\todo[inline]{Something about $A$ being symmetric. What about non-symmetric cases?}

\begin{figure}
  \input{images/corner_transfer_matrix_as_tensor_network.tikz}
  \caption{Corner transfer matrix expressed as a contraction of $a$-tensors.}
  \label{fig:corner_transfer_matrix_as_tensor_network}
\end{figure}

\subsection{Corner transfer matrix renormalization group method}
\todo[inline]{Say something about boundary conditions, also with TMRG algorithm.}

The algorithm proceeds very much like the transfer matrix renormalization group method. In addition to renormalizing the half-row transfer matrix $P$, we also renormalize the
corner transfer matrix $A$ at each step, using the projector obtained from diagonalizing
$A^4$.

\todo[inline]{Compare complexities of both algorithms.}

We first initialize $P_N$ and $A_N$, imposing boundary conditions as we see fit.

We then obtain the unrenormalized $A_{N+1}$ by adding a layer of spins to the quadrant
represented by $A_N$. This is done by contracting with two half-row transfer matrices
$P_N$ and a single $a$-tensor, as shown in the first step of
\autoref{fig:ctmrg_add_site_and_renormalize}. We obtain the unnormalized $P_{N+1}$ as
before, as shown in the first step of \autoref{fig:tmrg_add_site_and_renormalize}.

To find the optimal projector from $2m$ to $m$ basis states, we can directly diagonalize
$A_{N + 1}^4$, or, equivalently, $A_{N + 1}$. As always, we keep the basis states
corresponding to the $m$ largest eigenvectors. This is shown in
\autoref{fig:ctmrg_reduced_density_matrix}. We use the projector to obtain the
renormalized versions of $A_{N + 1}$ and $T_{N + 1}$
\begin{align}
    \widetilde{A}_{N + 1} &= OA_{N + 1}O^{\dagger}, \\
    \widetilde{T}_{N + 1} &= OT_{N + 1}O^{\dagger}.
\end{align}
shown in the second steps of \autoref{fig:ctmrg_add_site_and_renormalize} and
\autoref{fig:tmrg_add_site_and_renormalize}.

We repeat the above procedure to successively obtain
\begin{align}
  A_{N + 1} & \rightarrow A_{N + 2} \rightarrow \dots, \\
  T_{N + 1} & \rightarrow T_{N + 2} \rightarrow \dots
\end{align}
until a convergence criterion is reached.
\todo[inline]{Talk about convergence criterion.}

\begin{figure}
  \input{images/ctmrg_add_site_and_renormalize.tikz}
  \caption{In the first step, the unrenormalized $A_{N+1}$ is obtained by contracting with
  two copies of $P_N$ and a single $a$-tensor. This corresponds to adding a layer of spins
  to the quadrant, thus enlarging it from $N \times N$ to $N + 1 \times N + 1$. In the
  second step, $A_{N + 1}$ is renormalized with the projector obtained from diagonalizing
  $A_{N+1}^4$ and keeping the basis states corresponding to the $m$ largest eigenvalues.  }
  \label{fig:ctmrg_add_site_and_renormalize}
\end{figure}

\begin{figure}
  \input{images/ctmrg_reduced_density_matrix.tikz}
  \caption{The matrix $A_{N+1}^4$ is approximately equal to $\rho_{N+1}$ in
  \autoref{eq:reduced_density_matrix_classical_case}. Compare the graphical forms of this
  network and the one shown in \autoref{fig:tmrg_reduced_density_matrix}. We obtain the
  optimal projector by diagonalizing $A_{N + 1}^4$, or equivalently $A_{N+1}$.}
  \label{fig:ctmrg_reduced_density_matrix}
\end{figure}



\section{Calculation of observable quantities}

\todo[inline]{To do}

\section{Validity of approximation}

\todo[inline]{To do. Talk about spectrum of reduced density matrix/ctm and how it relates
to energy gap/correlation length and critical behaviour.}

\section{Equivalence to variational approximation in the space of matrix product states.}

\todo[inline]{Talk about Baxter and recent MPS forms. Maybe in appendix?}
